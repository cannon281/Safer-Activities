# Preprocessing Video to Extract Keypoints

This directory contains scripts for preprocessing video data to extract keypoints and prepare annotations for training models. 

## Step 1: Organizing Dataset

Ensure that your labels (CSVs) and videos are placed in the respective directories. The structure should be as follows:

- For non-wheelchair datasets: `data/normal/CSVs` and `data/normal/Videos`
- For wheelchair datasets: `data/wheelchair/CSVs` and `data/wheelchair/Videos`

Each CSV and its associated videos should be placed in these directories.

## Step 2: Extracting Keypoints

The main script for this step is `extract_keypoints.py`. Before running the script, make sure to set the `dataset_type` variable to either `"normal"` or `"wheelchair"`, depending on which dataset you intend to preprocess.

You can also adjust the `num_threads` variable to run multiple processes in parallel, depending on your GPU memory capacity.

Running `extract_keypoints.py` generates `.pkl` files for each video, containing annotations such as keypoints, scores, labels, bounding boxes, and more. These files are saved in the `data/{dataset_type}/annotations` directory.

## Dependencies for Keypoint Extraction

For extracting keypoints, we utilize YOLOv8x and ViTPose-H-Multi variant. We modified the repository at https://github.com/gpastal24/ViTPose-Pytorch for ViTPose inference.

## Step 3: Combining Annotations

After extracting keypoints, run `combine_pkls.py`. This script compiles all `.pkl` files in the `data/{dataset_type}/annotations` directory into a single file: `data/{dataset_type}/compiled_Annotations/aic_dataset_{dataset_type}_annotations.pkl`.

## Step 4: Adding Splits

To prepare the dataset for training, run `add_split_to_raw.py`. This script takes the compiled `.pkl` file from Step 3, adds subject and view splits to it, and saves the result as `data/{dataset_type}/compiled_Annotations/aic_{dataset_type}_dataset.pkl`.

## Step 5: Adjusting Labels (Optional)

If the labels need to be updated or readjusted, use `adjust_labels.py`. This script corrects the annotations in all `.pkl` files generated by `extract_keypoints.py` based on updated CSVs or new action mappings. This does not require the ViTPose and YOLO models to be rerun. 

After adjusting labels, rerun `combine_pkls.py` and `add_split_to_raw.py` to update the compiled dataset.

## Final Output

The final output (`aic_normal_dataset.pkl` or `aic_wheelchair_dataset.pkl`) contains the processed dataset ready for training models.

